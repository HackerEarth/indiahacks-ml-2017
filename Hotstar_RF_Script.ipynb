{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_json('train_data.json',orient=\"index\")\n",
    "test_data = pd.read_json('test_data.json',orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#set index\n",
    "train_data.reset_index(level = 0, inplace = True)\n",
    "train_data.rename(columns={'index':'ID'}, inplace=True)\n",
    "\n",
    "test_data.reset_index(level = 0, inplace = True)\n",
    "test_data.rename(columns={'index':'ID'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data has 200000 rows and 7 columns\n",
      "test_data data has 100000 rows and 6 columns\n"
     ]
    }
   ],
   "source": [
    "#check data\n",
    "print ('Train data has {} rows and {} columns'.format(train_data.shape[0],train_data.shape[1]))\n",
    "print ('test_data data has {} rows and {} columns'.format(test_data.shape[0],test_data.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Encode Target Variable\n",
    "train_data = train_data.replace({'segment':{'pos':1,'neg':0}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.923725\n",
       "1    0.076275\n",
       "Name: segment, dtype: float64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check target variable count\n",
    "train_data['segment'].value_counts()/train_data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating separate columns for genres and dow variables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data['g1'] = [re.sub(pattern='\\:\\d+',repl='',string=x) for x in train_data['genres']]\n",
    "train_data['g1'] = train_data['g1'].apply(lambda x: x.split(','))\n",
    "\n",
    "train_data['g2'] = [re.sub(pattern='\\:\\d+', repl='', string = x) for x in train_data['dow']]\n",
    "train_data['g2'] = train_data['g2'].apply(lambda x: x.split(','))\n",
    "\n",
    "t1 = pd.Series(train_data['g1']).apply(frozenset).to_frame(name='t_genre')\n",
    "t2 = pd.Series(train_data['g2']).apply(frozenset).to_frame(name='t_dow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# using frozenset trick - might take few minutes to process\n",
    "for t_genre in frozenset.union(*t1.t_genre):\n",
    "    t1[t_genre] = t1.apply(lambda _: int(t_genre in _.t_genre), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for t_dow in frozenset.union(*t2.t_dow):\n",
    "    t2[t_dow] = t2.apply(lambda _: int(t_dow in _.t_dow), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = pd.concat([train_data.reset_index(drop=True), t1], axis=1)\n",
    "train_data = pd.concat([train_data.reset_index(drop=True), t2], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data['g1'] = [re.sub(pattern='\\:\\d+',repl='',string=x) for x in test_data['genres']]\n",
    "test_data['g1'] = test_data['g1'].apply(lambda x: x.split(','))\n",
    "\n",
    "test_data['g2'] = [re.sub(pattern='\\:\\d+', repl='', string = x) for x in test_data['dow']]\n",
    "test_data['g2'] = test_data['g2'].apply(lambda x: x.split(','))\n",
    "\n",
    "t1_te = pd.Series(test_data['g1']).apply(frozenset).to_frame(name='t_genre')\n",
    "t2_te = pd.Series(test_data['g2']).apply(frozenset).to_frame(name='t_dow')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for t_genre in frozenset.union(*t1_te.t_genre):\n",
    "    t1_te[t_genre] = t1_te.apply(lambda _: int(t_genre in _.t_genre), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for t_dow in frozenset.union(*t2_te.t_dow):\n",
    "    t2_te[t_dow] = t2_te.apply(lambda _: int(t_dow in _.t_dow), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = pd.concat([test_data.reset_index(drop=True), t1_te], axis=1)\n",
    "test_data = pd.concat([test_data.reset_index(drop=True), t2_te], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sum of watch time from titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#the rows aren't list exactly. They are object, so we convert them to list and extract the watch time\n",
    "w1 = train_data['titles']\n",
    "w1 = w1.str.split(',')\n",
    "\n",
    "#create a nested list of numbers\n",
    "main = []\n",
    "for i in np.arange(train_data.shape[0]):\n",
    "    d1 = w1[i]\n",
    "    nest = []\n",
    "    nest = [re.sub(pattern = '.*\\:', repl=' ', string= d1[k]) for k in list(np.arange(len(d1)))]\n",
    "    main.append(nest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Turns out, there are blank values in the list, we need to fix them before we could add\n",
    "#### Fixing blanks in the list now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 blanks found\n",
      "1 blanks found\n",
      "2 blanks found\n",
      "3 blanks found\n",
      "4 blanks found\n",
      "5 blanks found\n",
      "6 blanks found\n",
      "7 blanks found\n",
      "8 blanks found\n",
      "9 blanks found\n",
      "10 blanks found\n",
      "11 blanks found\n",
      "12 blanks found\n",
      "13 blanks found\n",
      "14 blanks found\n",
      "15 blanks found\n",
      "16 blanks found\n",
      "17 blanks found\n",
      "18 blanks found\n",
      "19 blanks found\n",
      "20 blanks found\n",
      "21 blanks found\n",
      "22 blanks found\n",
      "23 blanks found\n",
      "24 blanks found\n",
      "25 blanks found\n",
      "26 blanks found\n",
      "27 blanks found\n",
      "28 blanks found\n",
      "29 blanks found\n",
      "30 blanks found\n",
      "31 blanks found\n",
      "32 blanks found\n",
      "33 blanks found\n",
      "34 blanks found\n",
      "35 blanks found\n",
      "36 blanks found\n",
      "37 blanks found\n",
      "38 blanks found\n",
      "39 blanks found\n",
      "40 blanks found\n",
      "41 blanks found\n",
      "42 blanks found\n",
      "43 blanks found\n",
      "44 blanks found\n",
      "45 blanks found\n"
     ]
    }
   ],
   "source": [
    "blanks = []\n",
    "for i in np.arange(len(main)):\n",
    "    if '' in main[i]:\n",
    "        print \"{} blanks found\".format(len(blanks))\n",
    "        blanks.append(i)\n",
    "        \n",
    "#replacing blanks with 0\n",
    "for i in blanks:\n",
    "    main[i] = [x.replace('','0') for x in main[i]]\n",
    "    \n",
    "#converting string to integers\n",
    "main = [[int(y) for y in x] for x in main]\n",
    "\n",
    "#adding the watch time\n",
    "tosum = []\n",
    "for i in np.arange(len(main)):\n",
    "    s = sum(main[i])\n",
    "    tosum.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data['title_sum'] = tosum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#making changes in test data\n",
    "w1_te = test_data['titles']\n",
    "w1_te = w1_te.str.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "main_te = []\n",
    "for i in np.arange(test_data.shape[0]):\n",
    "    d1 = w1_te[i]\n",
    "    nest = []\n",
    "    nest = [re.sub(pattern = '.*\\:', repl=' ', string= d1[k]) for k in list(np.arange(len(d1)))]\n",
    "    main_te.append(nest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 blanks found\n",
      "1 blanks found\n",
      "2 blanks found\n",
      "3 blanks found\n",
      "4 blanks found\n",
      "5 blanks found\n",
      "6 blanks found\n",
      "7 blanks found\n",
      "8 blanks found\n",
      "9 blanks found\n",
      "10 blanks found\n",
      "11 blanks found\n"
     ]
    }
   ],
   "source": [
    "blanks_te = []\n",
    "for i in np.arange(len(main_te)):\n",
    "    if '' in main_te[i]:\n",
    "        print \"{} blanks found\".format(len(blanks_te))\n",
    "        blanks_te.append(i)\n",
    "        \n",
    "#replacing blanks with 0\n",
    "for i in blanks_te:\n",
    "    main_te[i] = [x.replace('','0') for x in main_te[i]]\n",
    "    \n",
    "#converting string to integers\n",
    "main_te = [[int(y) for y in x] for x in main_te]\n",
    "\n",
    "#adding the watch time\n",
    "tosum_te = []\n",
    "for i in np.arange(len(main_te)):\n",
    "    s = sum(main_te[i])\n",
    "    tosum_te.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_data['title_sum'] = tosum_te"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create count variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#count variables\n",
    "def wcount(p):\n",
    "    return p.count(',')+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data['title_count'] = train_data['titles'].map(wcount)\n",
    "train_data['genres_count'] = train_data['genres'].map(wcount)\n",
    "train_data['cities_count'] = train_data['cities'].map(wcount)\n",
    "train_data['tod_count'] = train_data['tod'].map(wcount)\n",
    "train_data['dow_count'] = train_data['dow'].map(wcount)\n",
    "\n",
    "\n",
    "test_data['title_count'] = test_data['titles'].map(wcount)\n",
    "test_data['genres_count'] = test_data['genres'].map(wcount)\n",
    "test_data['cities_count'] = test_data['cities'].map(wcount)\n",
    "test_data['tod_count'] = test_data['tod'].map(wcount)\n",
    "test_data['dow_count'] = test_data['dow'].map(wcount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### remove variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_id = test_data['ID']\n",
    "train_data.drop(['ID','cities','dow','genres','titles','tod','g1','g2','t_genre','t_dow'], inplace=True, axis=1)\n",
    "test_data.drop(['ID','cities','dow','genres','titles','tod','g1','g2','t_genre','t_dow'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score, make_scorer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target = train_data['segment']\n",
    "train_data.drop('segment',axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## uncomment to do grid search - could get better score\n",
    "# X_train, X_val, y_train, y_val = train_test_split(train_data, target, train_size=0.6, random_state = 1)\n",
    "\n",
    "## doing grid search for parameters\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.grid_search import GridSearchCV\n",
    "# from sklearn.metrics import roc_auc_score, make_scorer\n",
    "# clf_scorer = make_scorer('roc_auc')\n",
    "# rfc = RandomForestClassifier(n_estimators=100,oob_score=True,)\n",
    "# param_grid = {\n",
    "#     'max_depth':[4,8,12],\n",
    "#     'max_features':['sqrt',10,15]\n",
    "    \n",
    "# }\n",
    "\n",
    "# cv_rfc = GridSearchCV(estimator=rfc, param_grid=param_grid, cv=5, scoring=clf_scorer)\n",
    "# cv_rfc.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=12, max_features=10, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=500, n_jobs=1, oob_score=False, random_state=None,\n",
       "            verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train final model\n",
    "rf_model = RandomForestClassifier(n_estimators=500,max_depth=12, max_features=10)\n",
    "rf_model.fit(train_data, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#make prediction\n",
    "rf_pred = rf_model.predict_proba(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#make submission file and submit\n",
    "columns = ['segment']\n",
    "sub = pd.DataFrame(data=rf_pred[:,1], columns=columns)\n",
    "sub['ID'] = test_id\n",
    "sub = sub[['ID','segment']]\n",
    "sub.to_csv(\"sub_hot.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
